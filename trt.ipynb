{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:module mmdet.models.VFNetHead not exist.\n",
      "WARNING:root:module mmdet.models.dense_heads.StageCascadeRPNHead not exist.\n",
      "WARNING:root:module mmdet.models.dense_heads.CascadeRPNHead not exist.\n",
      "WARNING:root:module mmdet.models.VFNet not exist.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/ubuntu/oljike/progs/mmdetection-to-tensorrt/mmdet2trt/converters/plugins/libamirstan_plugin.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6213a444d311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmdet2trt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmmdet2trt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# from torch2trt_dynamic import torch2trt_dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oljike/progs/mmdetection-to-tensorrt/mmdet2trt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/oljike/progs/mmdetection-to-tensorrt/mmdet2trt/converters/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_AnchorGeneratorDynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdelta2bbox_custom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_delta2bbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatched_nms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_batchednms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mRoiExtractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_roiextractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mConvWS2d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_ConvWS2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oljike/progs/mmdetection-to-tensorrt/mmdet2trt/converters/anchor_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_gridanchordynamic_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oljike/progs/mmdetection-to-tensorrt/mmdet2trt/converters/plugins/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcreate_gridanchordynamic_plugin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_gridanchordynamic_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcreate_batchednms_plugin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_batchednms_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcreate_delta2bbox_custom_plugin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_delta2bbox_custom_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcreate_dcn_plugin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dcn_plugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_dcnv2_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcreate_roiextractor_plugin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_roiextractor_plugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oljike/progs/mmdetection-to-tensorrt/mmdet2trt/converters/plugins/create_gridanchordynamic_plugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglobals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdir_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"libamirstan_plugin.so\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/open-mmlab/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/ubuntu/oljike/progs/mmdetection-to-tensorrt/mmdet2trt/converters/plugins/libamirstan_plugin.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import io\n",
    "import torch\n",
    "import torch.jit\n",
    "import torch.nn as nn\n",
    "from torch2trt import torch2trt, ConversionContext, TRTModule\n",
    "\n",
    "from mmdet.apis import init_detector, inference_detector, inference_batch_detector\n",
    "import tensorrt as trt\n",
    "from mmdet.datasets import replace_ImageToTensor\n",
    "from mmdet.datasets.pipelines import Compose\n",
    "from mmcv.parallel import collate, scatter\n",
    "from mmdet2trt import mmdet2trt\n",
    "# from torch2trt_dynamic import torch2trt_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     121
    ]
   },
   "outputs": [],
   "source": [
    "class JerseyModel(torch.nn.Module):\n",
    "    CHECKPOINT_FILENAME_PATTERN = 'model-{}.pth'\n",
    "\n",
    "    # __constants__ = ['_hidden1', '_hidden2', '_hidden3', '_hidden4', '_hidden5',\n",
    "    #                  '_hidden6', '_hidden7', '_hidden8', '_hidden9', '_hidden10',\n",
    "    #                  '_features', '_classifier',\n",
    "    #                  '_digit_length', '_digit1', '_digit2', '_digit3', '_digit4', '_digit5']\n",
    "\n",
    "    def __init__(self, inter_size=7):\n",
    "        super(JerseyModel, self).__init__()\n",
    "\n",
    "        self.inter_size = inter_size\n",
    "        self._hidden1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=48, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=48),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=48, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=160, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=160),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=160, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(num_features=192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self._hidden9 = nn.Sequential(\n",
    "            nn.Linear(192 * self.inter_size * self.inter_size, 3072),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self._hidden10 = nn.Sequential(\n",
    "            nn.Linear(3072, 3072),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self._digit_length = nn.Sequential(nn.Linear(3072, 4))\n",
    "        self._digit1 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self._digit2 = nn.Sequential(nn.Linear(3072, 11))\n",
    "\n",
    "\n",
    "    # @torch.jit.script_method\n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        x = self._hidden1(x)\n",
    "        x = self._hidden2(x)\n",
    "        x = self._hidden3(x)\n",
    "        x = self._hidden4(x)\n",
    "        x = self._hidden5(x)\n",
    "        x = self._hidden6(x)\n",
    "        x = self._hidden7(x)\n",
    "        x = self._hidden8(x)\n",
    "        # print(x.size())\n",
    "        x = x.view(x.size(0), 192 * self.inter_size * self.inter_size)\n",
    "        x = self._hidden9(x)\n",
    "        x = self._hidden10(x)\n",
    "\n",
    "        length_logits = self._digit_length(x)\n",
    "        digit1_logits = self._digit1(x)\n",
    "        digit2_logits = self._digit2(x)\n",
    "\n",
    "\n",
    "        return length_logits, digit1_logits, digit2_logits\n",
    "\n",
    "    def store(self, path_to_dir, step, maximum=5):\n",
    "        path_to_models = glob.glob(os.path.join(path_to_dir, Model.CHECKPOINT_FILENAME_PATTERN.format('*')))\n",
    "        if len(path_to_models) == maximum:\n",
    "            min_step = min([int(path_to_model.split('/')[-1][6:-4]) for path_to_model in path_to_models])\n",
    "            path_to_min_step_model = os.path.join(path_to_dir, Model.CHECKPOINT_FILENAME_PATTERN.format(min_step))\n",
    "            os.remove(path_to_min_step_model)\n",
    "\n",
    "        path_to_checkpoint_file = os.path.join(path_to_dir, Model.CHECKPOINT_FILENAME_PATTERN.format(step))\n",
    "        torch.save(self.state_dict(), path_to_checkpoint_file)\n",
    "        return path_to_checkpoint_file\n",
    "\n",
    "    def restore(self, path_to_checkpoint_file):\n",
    "        self.load_state_dict(torch.load(path_to_checkpoint_file))\n",
    "        step = int(path_to_checkpoint_file.split('/')[-1][6:-4])\n",
    "        return step\n",
    "    \n",
    "def _infer_jersey(model, numpy_image):\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        transform = transforms.Compose([\n",
    "            transforms.CenterCrop([54, 54]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        numpy_image = cv2.resize(numpy_image, (64, 64))\n",
    "\n",
    "        \n",
    "        image = Image.fromarray(numpy_image)\n",
    "        image = transform(image)\n",
    "        images = image.unsqueeze(dim=0).cuda()\n",
    "\n",
    "        length_logits, digit1_logits, digit2_logits= model.eval()(images)\n",
    "\n",
    "        length_prediction = length_logits.max(1)[1]\n",
    "        digit1_prediction = digit1_logits.max(1)[1]\n",
    "        digit2_prediction = digit2_logits.max(1)[1]\n",
    "  \n",
    "\n",
    "    return [digit1_prediction.item(), digit2_prediction.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_model = Model()\n",
    "class_model = JerseyModel(7)\n",
    "class_model.restore('../SVHNClassifier-PyTorch/work_dirs/basic_randaug/model-14000.pth')\n",
    "class_model = class_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example data\n",
    "x = torch.rand((1, 3, 54, 54)).cuda()\n",
    "\n",
    "# convert to TensorRT feeding sample data as input\n",
    "model_trt = torch2trt(class_model, [x], use_onnx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1912069320678711\n"
     ]
    }
   ],
   "source": [
    "x_trt = torch.rand((2, 3, 54, 54)).cuda()\n",
    "\n",
    "st = time.time()\n",
    "for i in range(1000):\n",
    "    r = model_trt(x_trt)\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9165523052215576\n"
     ]
    }
   ],
   "source": [
    "x_orig = torch.rand((2, 3, 54, 54)).cuda()\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "for i in range(1000):\n",
    "    r = class_model(x_orig)\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config and model weights path\n",
    "config_file = '/home/ubuntu/oljike/BallTracking/mmdetection/configs/yolo_jersey/yolov3_d53_320_273e_jersey.py'\n",
    "checkpoint_file = '/home/ubuntu/oljike/BallTracking/mmdetection/work_dirs/jersey_region_yolov3-320/epoch_80.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "det_model = init_detector(config_file, checkpoint_file, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def infer_det(model, imglist):\n",
    "    \n",
    "\n",
    "    is_batch = False\n",
    "    if isinstance(imglist, list):\n",
    "        is_batch = True\n",
    "    else:\n",
    "        imglist = [imglist]\n",
    "\n",
    "    cfg = model.cfg\n",
    "    device = next(model.parameters()).device  # model device\n",
    "    results = []\n",
    "\n",
    "    if isinstance(imglist[0], np.ndarray):\n",
    "        cfg = cfg.copy()\n",
    "        # set loading pipeline type\n",
    "        cfg.data.test.pipeline[0].type = 'LoadImageFromWebcam'\n",
    "\n",
    "    cfg.data.test.samples_per_gpu = len(imglist)\n",
    "    cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "    test_pipeline = Compose(cfg.data.test.pipeline)\n",
    "\n",
    "    datalist = []\n",
    "    for img in imglist:\n",
    "        # prepare data\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # directly add img\n",
    "            data = dict(img=img)\n",
    "        else:\n",
    "            # add information into dict\n",
    "            data = dict(img_info=dict(filename=img), img_prefix=None)\n",
    "        # build the data pipeline\n",
    "        data = test_pipeline(data)\n",
    "        datalist.append(data)\n",
    "\n",
    "    data = collate(datalist, samples_per_gpu=len(imglist))\n",
    "    # just get the actual data from DataContainer\n",
    "    data['img_metas'] = [img_metas.data[0] for img_metas in data['img_metas']]\n",
    "    data['img'] = [img.data[0] for img in data['img']]\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter to specified GPU\n",
    "        data = scatter(data, [device])[0]\n",
    "    else:\n",
    "        for m in model.modules():\n",
    "            assert not isinstance(\n",
    "                m, RoIPool\n",
    "            ), 'CPU inference with RoIPool is not supported currently.'\n",
    "    \n",
    "    return data\n",
    "        \n",
    "imglist = [np.ones((224,224,3)), np.ones((224,224,3))]\n",
    "data = infer_det(det_model, imglist)\n",
    "\n",
    "    # forward the model\n",
    "with torch.no_grad():\n",
    "    results = det_model(return_loss=False, rescale=True, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument unpacking (<ipython-input-61-2b30b48daf71>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-2b30b48daf71>\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    torch.onnx.export(module, **inputs, f, input_names=input_names, output_names=output_names)\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument unpacking\n"
     ]
    }
   ],
   "source": [
    "def default_input_names(num_inputs):\n",
    "    return [\"input_%d\" % i for i in range(num_inputs)]\n",
    "\n",
    "def default_output_names(num_outputs):\n",
    "    return [\"output_%d\" % i for i in range(num_outputs)]\n",
    "\n",
    "def convert(module, \n",
    "              inputs, \n",
    "              input_names=None, \n",
    "              output_names=None, \n",
    "              log_level=trt.Logger.ERROR, \n",
    "              max_batch_size=1,\n",
    "              fp16_mode=False, \n",
    "              max_workspace_size=1<<25, \n",
    "              strict_type_constraints=False, \n",
    "              keep_network=True, \n",
    "              int8_mode=False, \n",
    "              int8_calib_dataset=None,\n",
    "              int8_calib_algorithm=trt.CalibrationAlgoType.ENTROPY_CALIBRATION_2,\n",
    "              int8_calib_batch_size=1,\n",
    "              use_onnx=True):\n",
    "\n",
    "#     inputs_in = inputs\n",
    "\n",
    "#     # copy inputs to avoid modifications to source data\n",
    "#     inputs = [tensor.clone()[0:1] for tensor in inputs]  # only run single entry\n",
    "\n",
    "    logger = trt.Logger(log_level)\n",
    "    builder = trt.Builder(logger)\n",
    "    \n",
    "#     if isinstance(inputs, list):\n",
    "#         inputs = tuple(inputs)\n",
    "#     if not isinstance(inputs, tuple):\n",
    "#         inputs = (inputs,)\n",
    "        \n",
    "    # run once to get num outputs\n",
    "    outputs = module(return_loss=False, rescale=True, **inputs)\n",
    "    if not isinstance(outputs, tuple) and not isinstance(outputs, list):\n",
    "        outputs = (outputs,)\n",
    "        \n",
    "    if input_names is None:\n",
    "        input_names = default_input_names(len(inputs))\n",
    "    if output_names is None:\n",
    "        output_names = default_output_names(len(outputs))\n",
    "        \n",
    "    if use_onnx:\n",
    "        f = io.BytesIO()\n",
    "        torch.onnx.export(module, **inputs, f, input_names=input_names, output_names=output_names)\n",
    "        f.seek(0)\n",
    "        onnx_bytes = f.read()\n",
    "        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "        parser = trt.OnnxParser(network, logger)\n",
    "        parser.parse(onnx_bytes)\n",
    "\n",
    "\n",
    "    builder.max_workspace_size = max_workspace_size\n",
    "    builder.fp16_mode = fp16_mode\n",
    "    builder.max_batch_size = max_batch_size\n",
    "    builder.strict_type_constraints = strict_type_constraints\n",
    "\n",
    "\n",
    "    engine = builder.build_cuda_engine(network)\n",
    "    module_trt = TRTModule(engine, input_names, output_names)\n",
    "    \n",
    "    \n",
    "convert(det_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]]],\n",
       "\n",
       "\n",
       "        [[[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
       "\n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['img'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
